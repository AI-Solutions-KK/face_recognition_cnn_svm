{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0fee7d45c8184a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T04:31:44.063616Z",
     "start_time": "2025-11-12T04:31:44.039617Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 1Ô∏è‚É£ Create a new virtual environment (name it 'venv_tf' or anything)\n",
    "# !python -m venv venv_face_identify\n",
    "#\n",
    "# # 2Ô∏è‚É£ Activate it (Windows-specific)\n",
    "# !venv_face_identify\\Scripts\\activate\n",
    "#\n",
    "# # 3Ô∏è‚É£ Upgrade pip and install Jupyter into this venv\n",
    "# !venv_face_identify\\Scripts\\python -m pip install --upgrade pip\n",
    "# !venv_face_identify\\Scripts\\python -m pip install jupyter ipykernel\n",
    "#\n",
    "# # 4Ô∏è‚É£ Add this environment as a Jupyter kernel\n",
    "# !venv_face_identify\\Scripts\\python -m ipykernel install --user --name=venv_face_identify --display-name \"Python (venv_face_identify)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "976cf377ddb09bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install numpy pandas matplotlib tensorflow torch facenet-pytorch scikit-learn opencv-python tqdm pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9804867f58b98631",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T17:31:25.502299Z",
     "start_time": "2025-11-13T17:31:25.046628Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_ROOT: human_face_dataset/pins_face_recognition\n"
     ]
    }
   ],
   "source": [
    "# BLOCK 1 ‚Äî Configuration Block (edit only DATA_ROOT)\n",
    "# Directory/Path Define\n",
    "\n",
    "DATA_ROOT = \"human_face_dataset/pins_face_recognition\"  # << set this to your dataset folder (one subfolder per person)\n",
    "# Block 1 ‚Äî Imports & configuration\n",
    "from pathlib import Path\n",
    "import os, time, pickle, hashlib\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# ---------- EDIT IF NEEDED ----------\n",
    "# Set DATA_ROOT to the folder that contains one folder per person (images inside)\n",
    "DATA_ROOT = \"human_face_dataset/pins_face_recognition\"\n",
    "# ------------------------------------\n",
    "\n",
    "# Cache & artifact locations\n",
    "CACHE_DIR = Path('./embeddings_cache'); CACHE_DIR.mkdir(exist_ok=True)\n",
    "EMB_FILE = CACHE_DIR / 'X_emb.npy'\n",
    "LBL_FILE = CACHE_DIR / 'y_lbl.npy'\n",
    "PATHS_FILE = CACHE_DIR / 'paths.npy'\n",
    "CLF_FILE = CACHE_DIR / 'svc_model_retrained.pkl'\n",
    "CENTROIDS_FILE = CACHE_DIR / 'centroids.npy'\n",
    "CLASSES_FILE = CACHE_DIR / 'classes.npy'\n",
    "\n",
    "# Image extensions considered\n",
    "EXTS = {'.jpg', '.jpeg', '.png'}\n",
    "print(\"DATA_ROOT:\", DATA_ROOT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79caa6dce9652eb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T17:31:46.783174Z",
     "start_time": "2025-11-13T17:31:30.268547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models ready. Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Block 2 ‚Äî Imports and Model initialization (facenet-pytorch)\n",
    "try:\n",
    "    from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "    import torch\n",
    "    import cv2\n",
    "    from PIL import Image\n",
    "except Exception as e:\n",
    "    raise ImportError(\"Missing packages. Run pip install facenet-pytorch torch torchvision opencv-python scikit-learn tqdm matplotlib pillow\") from e\n",
    "\n",
    "device = 'cpu'\n",
    "mtcnn = MTCNN(keep_all=False, device=device)          # detector + alignment\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval()  # embedding model (512-d)\n",
    "print(\"Models ready. Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9ee095eaffe8042",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T17:31:50.443610Z",
     "start_time": "2025-11-13T17:31:50.384356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved 0 duplicate files to duplicates. Kept first occurrence of each duplicated image.\n"
     ]
    }
   ],
   "source": [
    "# BLOCK 3: Move duplicate files into ./duplicates/ (safe)\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# `dupes` is the dict you produced mapping md5 -> [paths], or re-find below\n",
    "def find_duplicates(paths):\n",
    "    import hashlib\n",
    "    from collections import defaultdict\n",
    "    def h(p):\n",
    "        m = hashlib.md5()\n",
    "        with open(p,'rb') as f:\n",
    "            for chunk in iter(lambda: f.read(8192), b''):\n",
    "                m.update(chunk)\n",
    "        return m.hexdigest()\n",
    "    D = defaultdict(list)\n",
    "    for p in paths:\n",
    "        D[h(p)].append(p)\n",
    "    return {k:v for k,v in D.items() if len(v)>1}\n",
    "\n",
    "# Build paths list (adjust root)\n",
    "root = Path(r'D:\\DATA_SCIENCE\\My_Projects\\testing\\human_face_identify\\pins_face_recognition')\n",
    "paths = [str(p) for p in root.rglob('*') if p.suffix.lower() in ('.jpg','.jpeg','.png')]\n",
    "dupes = find_duplicates(paths)\n",
    "\n",
    "outdir = Path('./duplicates'); outdir.mkdir(exist_ok=True)\n",
    "moved = 0\n",
    "for h, lst in dupes.items():\n",
    "    keep = lst[0]            # keep first occurrence\n",
    "    for p in lst[1:]:\n",
    "        target = outdir / Path(p).name\n",
    "        # avoid name collisions\n",
    "        i = 1\n",
    "        while target.exists():\n",
    "            target = outdir / f\"{Path(p).stem}_{i}{Path(p).suffix}\"\n",
    "            i += 1\n",
    "        shutil.move(p, str(target))\n",
    "        moved += 1\n",
    "print(f\"Moved {moved} duplicate files to {outdir}. Kept first occurrence of each duplicated image.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "972ac90a401e9093",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T17:31:56.802589Z",
     "start_time": "2025-11-13T17:31:55.752439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found: 105 Total images: 17534\n",
      "Saved paths to embeddings_cache\\paths.npy\n"
     ]
    }
   ],
   "source": [
    "# Block 4 ‚Äî Data Count & Define Image Path for the operation\n",
    "# Build image_paths and labels and save to cache (paths.npy)\n",
    "\n",
    "from pathlib import Path\n",
    "ROOT = Path(DATA_ROOT)\n",
    "image_paths = [str(p) for p in sorted(ROOT.rglob('*')) if p.suffix.lower() in EXTS]\n",
    "labels = [Path(p).parent.name for p in image_paths]\n",
    "print(\"Classes found:\", len(set(labels)), \"Total images:\", len(image_paths))\n",
    "# Save list of paths to allow deduping later or resume\n",
    "np.save(PATHS_FILE, np.array(image_paths, dtype=object))\n",
    "print(\"Saved paths to\", PATHS_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fc6980927c76bde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T17:32:07.204998Z",
     "start_time": "2025-11-13T17:32:07.047296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from cache: 17534 embeddings loaded.\n",
      "No new images to process.\n"
     ]
    }
   ],
   "source": [
    "# Block 5: (Embedding Block)\n",
    "# Batched, resumable extraction to EMB_FILE / LBL_FILE (overwrite/resume)\n",
    "\n",
    "import numpy as np, torch\n",
    "from PIL import Image\n",
    "from math import ceil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# parameters: tune for your machine\n",
    "BATCH_SIZE = 48     # try 16/32/48/64 depending on RAM\n",
    "SAVE_EVERY = 1      # save after this many batches\n",
    "MAX_SIDE = 640      # resize max side for speed; lower to 480 if needed\n",
    "\n",
    "# load list of all paths\n",
    "image_paths = list(np.load(PATHS_FILE, allow_pickle=True))\n",
    "n_total = len(image_paths)\n",
    "\n",
    "# resume from cache if exists\n",
    "if EMB_FILE.exists() and LBL_FILE.exists():\n",
    "    X_cached = np.load(EMB_FILE)\n",
    "    y_cached = np.load(LBL_FILE, allow_pickle=True)\n",
    "    start_idx = len(y_cached)\n",
    "    X_list = [X_cached[i] for i in range(len(X_cached))]\n",
    "    y_list = [y_cached[i] for i in range(len(y_cached))]\n",
    "    print(f\"Resuming from cache: {start_idx} embeddings loaded.\")\n",
    "else:\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    start_idx = 0\n",
    "\n",
    "def safe_open_resize(path, max_side=MAX_SIDE):\n",
    "    try:\n",
    "        im = Image.open(path).convert('RGB')\n",
    "        w,h = im.size\n",
    "        s = max(w,h)\n",
    "        if s > max_side:\n",
    "            scale = max_side / s\n",
    "            im = im.resize((int(w*scale), int(h*scale)), Image.BILINEAR)\n",
    "        return im\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "pairs = image_paths[start_idx:]\n",
    "n = len(pairs)\n",
    "if n == 0:\n",
    "    print(\"No new images to process.\")\n",
    "else:\n",
    "    n_batches = ceil(n / BATCH_SIZE)\n",
    "    print(f\"Processing {n} images in {n_batches} batches (batch_size={BATCH_SIZE})\")\n",
    "    bad_files = []\n",
    "    batch_count = 0\n",
    "    t_total = time.time()\n",
    "    for b in tqdm(range(n_batches), desc='Batches'):\n",
    "        s = b * BATCH_SIZE\n",
    "        e = min(s + BATCH_SIZE, n)\n",
    "        batch_paths = pairs[s:e]\n",
    "        pil_imgs = []\n",
    "        pil_labels = []\n",
    "        real_paths = []\n",
    "        for path in batch_paths:\n",
    "            im = safe_open_resize(path)\n",
    "            if im is None:\n",
    "                bad_files.append(path)\n",
    "                continue\n",
    "            pil_imgs.append(im)\n",
    "            pil_labels.append(Path(path).parent.name)\n",
    "            real_paths.append(path)\n",
    "        if not pil_imgs:\n",
    "            batch_count += 1\n",
    "            continue\n",
    "\n",
    "        # try batch detection; fallback to per-image if needed\n",
    "        faces = None\n",
    "        try:\n",
    "            faces = mtcnn(pil_imgs)\n",
    "        except Exception as ex:\n",
    "            faces = None\n",
    "            # fallback will run below\n",
    "\n",
    "        face_tensors = []\n",
    "        valid_labels = []\n",
    "        if isinstance(faces, torch.Tensor):\n",
    "            # tensor -> all faces detected and aligned\n",
    "            for i in range(faces.shape[0]):\n",
    "                f = faces[i].unsqueeze(0)\n",
    "                face_tensors.append(f)\n",
    "                valid_labels.append(pil_labels[i])\n",
    "        else:\n",
    "            # fallback to per-image detection\n",
    "            for im, lab, path in zip(pil_imgs, pil_labels, real_paths):\n",
    "                try:\n",
    "                    f = mtcnn(im)\n",
    "                    if f is None:\n",
    "                        bad_files.append(path)\n",
    "                        continue\n",
    "                    if f.dim() == 3:\n",
    "                        f = f.unsqueeze(0)\n",
    "                    face_tensors.append(f)\n",
    "                    valid_labels.append(lab)\n",
    "                except Exception:\n",
    "                    bad_files.append(path)\n",
    "                    continue\n",
    "\n",
    "        if not face_tensors:\n",
    "            batch_count += 1\n",
    "            if batch_count % SAVE_EVERY == 0 and len(y_list) > 0:\n",
    "                np.save(EMB_FILE, np.vstack(X_list))\n",
    "                np.save(LBL_FILE, np.array(y_list, dtype=object))\n",
    "            continue\n",
    "\n",
    "        face_batch = torch.cat(face_tensors, dim=0)\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                emb_batch = resnet(face_batch).cpu().numpy()\n",
    "        except Exception:\n",
    "            # per-item embedding fallback\n",
    "            emb_batch = []\n",
    "            for ft in face_tensors:\n",
    "                try:\n",
    "                    with torch.no_grad():\n",
    "                        e = resnet(ft).cpu().numpy().reshape(-1)\n",
    "                    emb_batch.append(e)\n",
    "                except Exception:\n",
    "                    emb_batch.append(None)\n",
    "            filtered = [(e, lab) for e, lab in zip(emb_batch, valid_labels) if e is not None]\n",
    "            if not filtered:\n",
    "                batch_count += 1\n",
    "                continue\n",
    "            emb_batch = np.vstack([f[0] for f in filtered])\n",
    "            valid_labels = [f[1] for f in filtered]\n",
    "\n",
    "        for emb, lab in zip(emb_batch, valid_labels):\n",
    "            X_list.append(emb.astype('float32'))\n",
    "            y_list.append(lab)\n",
    "\n",
    "        batch_count += 1\n",
    "        if batch_count % SAVE_EVERY == 0:\n",
    "            np.save(EMB_FILE, np.vstack(X_list))\n",
    "            np.save(LBL_FILE, np.array(y_list, dtype=object))\n",
    "            t_elapsed = time.time() - t_total\n",
    "            processed = len(y_list)\n",
    "            rate = processed / t_elapsed if t_elapsed > 0 else 0\n",
    "            print(f\"Saved cache: {len(y_list)} embeddings; rate {rate:.2f} emb/s\")\n",
    "\n",
    "    # final save\n",
    "    if len(X_list) > 0:\n",
    "        X = np.vstack(X_list).astype('float32')\n",
    "        y = np.array(y_list, dtype=object)\n",
    "        np.save(EMB_FILE, X)\n",
    "        np.save(LBL_FILE, y)\n",
    "        np.save(PATHS_FILE, np.array(image_paths, dtype=object))  # ensure paths saved\n",
    "        print(\"Done. Extracted embeddings:\", X.shape)\n",
    "    else:\n",
    "        print(\"No embeddings extracted in this run.\")\n",
    "\n",
    "    if bad_files:\n",
    "        bad_txt = CACHE_DIR / 'bad_files.txt'\n",
    "        with open(bad_txt, 'w', encoding='utf-8') as f:\n",
    "            for p in sorted(set(bad_files)):\n",
    "                f.write(p + \"\\n\")\n",
    "        print(f\"{len(set(bad_files))} problematic files logged to {bad_txt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "109a7be90dac3b1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T17:32:41.627489Z",
     "start_time": "2025-11-13T17:32:22.400175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (17534, 512) 17534\n",
      "Unique images: 17533 of 17534\n",
      "Saved deduplicated embeddings to embeddings_cache/*.\n"
     ]
    }
   ],
   "source": [
    "# Block 6 ‚Äî deduplicate embeddings using file MD5 (optional)\n",
    "if not EMB_FILE.exists() or not PATHS_FILE.exists():\n",
    "    print(\"Embeddings or paths missing. Run Block 5 first.\")\n",
    "else:\n",
    "    X = np.load(EMB_FILE)\n",
    "    y = np.load(LBL_FILE, allow_pickle=True)\n",
    "    paths = list(np.load(PATHS_FILE, allow_pickle=True))\n",
    "    print(\"Loaded:\", X.shape, len(paths))\n",
    "    seen = {}\n",
    "    keep_idx = []\n",
    "    for i, p in enumerate(paths):\n",
    "        try:\n",
    "            h = hashlib.md5(open(p, 'rb').read()).hexdigest()\n",
    "        except Exception:\n",
    "            continue\n",
    "        if h in seen:\n",
    "            continue\n",
    "        seen[h] = i\n",
    "        keep_idx.append(i)\n",
    "    print(\"Unique images:\", len(keep_idx), \"of\", len(paths))\n",
    "    X_new = X[keep_idx]\n",
    "    y_new = y[keep_idx]\n",
    "    paths_new = [paths[i] for i in keep_idx]\n",
    "    np.save(CACHE_DIR / 'X_emb_dedup.npy', X_new)\n",
    "    np.save(CACHE_DIR / 'y_lbl_dedup.npy', y_new)\n",
    "    np.save(CACHE_DIR / 'paths_dedup.npy', np.array(paths_new, dtype=object))\n",
    "    print(\"Saved deduplicated embeddings to embeddings_cache/*.\")\n",
    "    # If you want to use deduped files as main, replace originals (manual step)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "################################################################################\n",
    "# Block 6A - Check Class Balance (FAST - no processing needed)\n",
    "################################################################################\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Load existing embeddings and labels\n",
    "X = np.load(EMB_FILE)\n",
    "y = np.load(LBL_FILE, allow_pickle=True)\n",
    "\n",
    "# Count samples per class\n",
    "class_counts = Counter(y)\n",
    "class_counts_sorted = sorted(class_counts.items(), key=lambda x: x[1])\n",
    "\n",
    "# Statistics\n",
    "min_samples = min(class_counts.values())\n",
    "max_samples = max(class_counts.values())\n",
    "mean_samples = np.mean(list(class_counts.values()))\n",
    "median_samples = np.median(list(class_counts.values()))\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä CLASS BALANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total classes: {len(class_counts)}\")\n",
    "print(f\"Total samples: {len(y)}\")\n",
    "print(f\"Min samples per class: {min_samples}\")\n",
    "print(f\"Max samples per class: {max_samples}\")\n",
    "print(f\"Mean samples per class: {mean_samples:.1f}\")\n",
    "print(f\"Median samples per class: {median_samples:.1f}\")\n",
    "print(f\"Imbalance ratio: {max_samples/min_samples:.2f}x\")\n",
    "\n",
    "# Define balance threshold\n",
    "BALANCE_THRESHOLD = 1.5  # If max/min > 1.5x, consider imbalanced\n",
    "IS_BALANCED = (max_samples / min_samples) <= BALANCE_THRESHOLD\n",
    "\n",
    "if IS_BALANCED:\n",
    "    print(f\"\\n‚úÖ Dataset is BALANCED (ratio {max_samples/min_samples:.2f}x ‚â§ {BALANCE_THRESHOLD}x)\")\n",
    "    print(\"   No augmentation needed!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Dataset is IMBALANCED (ratio {max_samples/min_samples:.2f}x > {BALANCE_THRESHOLD}x)\")\n",
    "    print(\"   Augmentation recommended! Run Block 6B below.\")\n",
    "\n",
    "# Show classes with fewest samples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìâ CLASSES WITH FEWEST SAMPLES (Bottom 10)\")\n",
    "print(\"=\"*80)\n",
    "for class_name, count in class_counts_sorted[:10]:\n",
    "    print(f\"  {class_name}: {count} samples\")\n",
    "\n",
    "# Show classes with most samples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìà CLASSES WITH MOST SAMPLES (Top 10)\")\n",
    "print(\"=\"*80)\n",
    "for class_name, count in class_counts_sorted[-10:]:\n",
    "    print(f\"  {class_name}: {count} samples\")\n",
    "\n",
    "# Visualize distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "counts = [count for _, count in class_counts_sorted]\n",
    "plt.bar(range(len(counts)), counts, color='skyblue', edgecolor='black')\n",
    "plt.axhline(y=mean_samples, color='red', linestyle='--', label=f'Mean: {mean_samples:.1f}')\n",
    "plt.axhline(y=median_samples, color='green', linestyle='--', label=f'Median: {median_samples:.1f}')\n",
    "plt.xlabel('Class Index (sorted by count)')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Samples per Class Distribution')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('class_distribution.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Distribution plot saved as: class_distribution.png\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save balance report\n",
    "balance_report = []\n",
    "for class_name, count in sorted(class_counts.items()):\n",
    "    balance_report.append({\n",
    "        'class': class_name,\n",
    "        'samples': count,\n",
    "        'percentage': f\"{count/len(y)*100:.2f}%\",\n",
    "        'status': '‚úì Good' if count >= mean_samples*0.7 else '‚ö† Low'\n",
    "    })\n",
    "\n",
    "balance_df = pd.DataFrame(balance_report)\n",
    "balance_df.to_csv('class_balance_report.csv', index=False)\n",
    "print(f\"‚úÖ Detailed balance report saved as: class_balance_report.csv\")\n",
    "\n"
   ],
   "id": "9d7ca76416cc676c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "################################################################################\n",
    "# Block 6B - Smart Augmentation (Embedding-Level) (cache-aware incremental augment)\n",
    "################################################################################\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"üîÑ SMART AUGMENTATION (Embedding-Level)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ---- imports ----\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import sys, os\n",
    "\n",
    "# ---- Helpers: locate CACHE_DIR (try existing variable else default) ----\n",
    "try:\n",
    "    CACHE_DIR\n",
    "except NameError:\n",
    "    CACHE_DIR = Path('embeddings_cache')\n",
    "CACHE_DIR = Path(CACHE_DIR)\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# candidate paths (try to preserve earlier names if present in workspace)\n",
    "emb_cache = CACHE_DIR / 'X_emb_augmented.npy'\n",
    "lbl_cache = CACHE_DIR / 'y_lbl_augmented.npy'\n",
    "paths_cache = CACHE_DIR / 'paths_augmented.npy'\n",
    "augmentation_report_csv = Path('augmentation_report.csv')\n",
    "\n",
    "# Try loading embeddings/labels/paths either from workspace variables or files\n",
    "def _load_var_or_file(varnames, candidates, label):\n",
    "    g = globals()\n",
    "    for n in varnames:\n",
    "        if n in g and g[n] is not None:\n",
    "            return g[n], f\"variable '{n}'\"\n",
    "    for c in candidates:\n",
    "        if c is None:\n",
    "            continue\n",
    "        p = Path(c)\n",
    "        if p.exists():\n",
    "            val = np.load(p, allow_pickle=True)\n",
    "            return val, f\"file: {p}\"\n",
    "    raise NameError(f\"{label} not found (tried vars {varnames} and files {candidates})\")\n",
    "\n",
    "# Attempt to find X, y, paths in globals or in common files\n",
    "emb_candidates = [emb_cache, 'X_emb.npy', 'X.npy', 'embeddings.npy']\n",
    "lbl_candidates = [lbl_cache, 'y_lbl.npy', 'y.npy', 'labels.npy']\n",
    "paths_candidates = [paths_cache, 'paths.npy']\n",
    "\n",
    "try:\n",
    "    X, X_source = _load_var_or_file(['X', 'X_emb', 'embeddings', 'embs'], emb_candidates, \"embeddings (X)\")\n",
    "    print(f\"‚úÖ Loaded embeddings X from {X_source}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(str(e) + \"\\nMake sure embeddings exist in memory as X or as a file.\") from None\n",
    "\n",
    "try:\n",
    "    y, y_source = _load_var_or_file(['y', 'y_lbl', 'labels', 'y_labels'], lbl_candidates, \"labels (y)\")\n",
    "    print(f\"‚úÖ Loaded labels y from {y_source}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(str(e) + \"\\nMake sure labels exist in memory as y or as a file.\") from None\n",
    "\n",
    "try:\n",
    "    paths, paths_source = _load_var_or_file(['paths', 'img_paths', 'paths_arr'], paths_candidates, \"paths\")\n",
    "    print(f\"‚úÖ Loaded 'paths' from {paths_source}\")\n",
    "except Exception:\n",
    "    # fallback: create dummy paths (so code runs)\n",
    "    n_items = len(y) if 'y' in globals() else len(X)\n",
    "    paths = np.array([f\"idx_{i}\" for i in range(n_items)], dtype=object)\n",
    "    print(\"‚ö†Ô∏è 'paths' not found ‚Äî created placeholder path names.\")\n",
    "\n",
    "# ---- compute current class stats (base dataset) ----\n",
    "unique_classes, counts = np.unique(y, return_counts=True)\n",
    "min_samples = int(counts.min())\n",
    "max_samples = int(counts.max())\n",
    "imb_ratio = max_samples / min_samples\n",
    "IS_BALANCED = imb_ratio <= 1.5\n",
    "\n",
    "if not IS_BALANCED:\n",
    "    print(\"‚ö†Ô∏è Your dataset is imbalanced. Proceeding with augmentation...\")\n",
    "    PROCEED_WITH_AUGMENTATION = True\n",
    "else:\n",
    "    print(\"‚úÖ Dataset is balanced. Skipping augmentation.\")\n",
    "    PROCEED_WITH_AUGMENTATION = False\n",
    "\n",
    "if PROCEED_WITH_AUGMENTATION:\n",
    "    # ---- configuration / performance ----\n",
    "    TARGET_SAMPLES = max_samples              # dynamic: balance to current max class size\n",
    "    MIN_SAMPLES_FOR_AUGMENT = 150             # informational only\n",
    "    AUG_BATCH = 128                           # batch size for synthetic generation\n",
    "    NOISE_STD = 0.01                          # gaussian noise stddev added to synthetic\n",
    "    print(f\"üéØ Target: {TARGET_SAMPLES} samples per class  (dynamic)\")\n",
    "    print(f\"üìä Current range: {min_samples} - {max_samples} samples\")\n",
    "    print(f\"üîß Will augment classes with < {TARGET_SAMPLES} samples (actual target)\")\n",
    "\n",
    "    # If augmented cache exists, load it. We will check its balance and possibly incrementally augment.\n",
    "    if emb_cache.exists() and lbl_cache.exists() and paths_cache.exists():\n",
    "        print(\"‚è±Ô∏è Augmented cache detected ‚Äî loading augmented artifacts from cache.\")\n",
    "        X_augmented = np.load(emb_cache, allow_pickle=True)\n",
    "        y_augmented = np.load(lbl_cache, allow_pickle=True)\n",
    "        paths_augmented = np.load(paths_cache, allow_pickle=True)\n",
    "\n",
    "        # Compute balance on cached augmented data\n",
    "        cached_counts = Counter(y_augmented)\n",
    "        cached_min = min(cached_counts.values())\n",
    "        cached_max = max(cached_counts.values())\n",
    "        print(f\"\\nCached augmented data range: {cached_min} - {cached_max} samples (imbalance {cached_max/cached_min:.2f}x)\")\n",
    "\n",
    "        # If cache is already fully balanced to TARGET_SAMPLES, skip. Otherwise do incremental augmentation.\n",
    "        if cached_min >= TARGET_SAMPLES and cached_max <= TARGET_SAMPLES:\n",
    "            print(\"‚úÖ Cached augmented dataset already meets TARGET_SAMPLES for all classes. No further augmentation required.\")\n",
    "        else:\n",
    "            # We'll only generate the missing samples per class to reach TARGET_SAMPLES\n",
    "            print(\"üîÅ Cached dataset still imbalanced vs TARGET_SAMPLES ‚Äî performing incremental augmentation for missing classes...\")\n",
    "            incremental_stats = []\n",
    "\n",
    "            # Build a dictionary mapping class -> current count in cached set\n",
    "            class_to_count = {c: cached_counts.get(c, 0) for c in unique_classes}\n",
    "\n",
    "            # For speed, convert X_augmented to list for append, same for labels/paths\n",
    "            X_list = list(X_augmented)\n",
    "            y_list = list(y_augmented)\n",
    "            paths_list = list(paths_augmented)\n",
    "\n",
    "            for class_name in tqdm(unique_classes, desc='Processing classes for incremental augmentation'):\n",
    "                current = class_to_count.get(class_name, 0)\n",
    "                if current >= TARGET_SAMPLES:\n",
    "                    continue  # already enough\n",
    "                needed = TARGET_SAMPLES - current\n",
    "                synthetic_count = 0\n",
    "\n",
    "                # Get originals for this class from the original X,y (not from cache) ‚Äî better variation\n",
    "                mask_orig = (y == class_name)\n",
    "                orig_embs = X[mask_orig]\n",
    "                if len(orig_embs) == 0:\n",
    "                    # if original dataset had 0, fallback to using class embeddings from cached set\n",
    "                    mask_cache = (y_augmented == class_name)\n",
    "                    orig_embs = X_augmented[mask_cache]\n",
    "                    if len(orig_embs) == 0:\n",
    "                        print(f\"‚ö†Ô∏è No originals found for class '{class_name}' in either original or cached data. Skipping.\")\n",
    "                        continue\n",
    "\n",
    "                emb_indices = np.arange(len(orig_embs))\n",
    "                synth_idx = 0\n",
    "\n",
    "                while synthetic_count < needed:\n",
    "                    batch = int(min(AUG_BATCH, needed - synthetic_count))\n",
    "\n",
    "                    if len(emb_indices) == 1:\n",
    "                        idx1 = idx2 = np.zeros(batch, dtype=int)\n",
    "                    else:\n",
    "                        idx1 = np.random.choice(emb_indices, size=batch, replace=True)\n",
    "                        idx2 = np.random.choice(emb_indices, size=batch, replace=True)\n",
    "\n",
    "                    emb1 = orig_embs[idx1]\n",
    "                    emb2 = orig_embs[idx2]\n",
    "\n",
    "                    alphas = np.random.uniform(0.3, 0.7, size=(batch, 1))\n",
    "                    synthetics = alphas * emb1 + (1 - alphas) * emb2\n",
    "                    noise = np.random.normal(0.0, NOISE_STD, size=synthetics.shape)\n",
    "                    synthetics = synthetics + noise\n",
    "\n",
    "                    norms = np.linalg.norm(synthetics, axis=1, keepdims=True)\n",
    "                    norms[norms == 0] = 1.0\n",
    "                    synthetics = synthetics / norms\n",
    "\n",
    "                    for i in range(batch):\n",
    "                        X_list.append(synthetics[i].astype('float32'))\n",
    "                        y_list.append(class_name)\n",
    "                        paths_list.append(f\"incremental_synth_{class_name}_{synth_idx}\")\n",
    "                        synth_idx += 1\n",
    "\n",
    "                    synthetic_count += batch\n",
    "\n",
    "                incremental_stats.append({\n",
    "                    'class': class_name,\n",
    "                    'original_in_cache': current,\n",
    "                    'added': synthetic_count,\n",
    "                    'new_total': current + synthetic_count\n",
    "                })\n",
    "                # update cached count for this class so subsequent checks use updated value\n",
    "                class_to_count[class_name] = current + synthetic_count\n",
    "\n",
    "            # Convert lists back to arrays\n",
    "            X_augmented = np.array(X_list, dtype='float32')\n",
    "            y_augmented = np.array(y_list, dtype=object)\n",
    "            paths_augmented = np.array(paths_list, dtype=object)\n",
    "\n",
    "            # Update augmentation report: combine previous report (if exists) with incremental report\n",
    "            if augmentation_report_csv.exists():\n",
    "                try:\n",
    "                    prev_df = pd.read_csv(augmentation_report_csv)\n",
    "                except Exception:\n",
    "                    prev_df = pd.DataFrame()\n",
    "            else:\n",
    "                prev_df = pd.DataFrame()\n",
    "\n",
    "            if incremental_stats:\n",
    "                inc_df = pd.DataFrame(incremental_stats)\n",
    "                # If prev_df has same classes, add added counts to previous synthetic; otherwise concat\n",
    "                if not prev_df.empty:\n",
    "                    prev_df = prev_df.set_index('class')\n",
    "                    inc_df = inc_df.set_index('class')\n",
    "                    for cls in inc_df.index:\n",
    "                        if cls in prev_df.index:\n",
    "                            prev_row = prev_df.loc[cls].to_dict()\n",
    "                            # update synthetic & total\n",
    "                            prev_df.at[cls, 'synthetic'] = int(prev_df.at[cls, 'synthetic']) + int(inc_df.at[cls, 'added'])\n",
    "                            prev_df.at[cls, 'total'] = int(prev_df.at[cls, 'total']) + int(inc_df.at[cls, 'added'])\n",
    "                        else:\n",
    "                            prev_df = pd.concat([prev_df, pd.DataFrame({\n",
    "                                'original': [inc_df.at[cls, 'original_in_cache']],\n",
    "                                'synthetic': [inc_df.at[cls, 'added']],\n",
    "                                'total': [inc_df.at[cls, 'new_total']]\n",
    "                            }, index=[cls])])\n",
    "                    prev_df = prev_df.reset_index().rename(columns={'index': 'class'})\n",
    "                    final_report_df = prev_df\n",
    "                else:\n",
    "                    inc_df = inc_df.reset_index().rename(columns={'index': 'class'})\n",
    "                    final_report_df = inc_df\n",
    "                final_report_df.to_csv(augmentation_report_csv, index=False)\n",
    "                print(\"\\n‚úÖ Incremental augmentation completed and augmentation_report.csv updated.\")\n",
    "                try:\n",
    "                    print(final_report_df.to_string(index=False))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            else:\n",
    "                print(\"‚ÑπÔ∏è No incremental augmentation was needed (cache already met TARGET_SAMPLES).\")\n",
    "\n",
    "            # Save updated caches\n",
    "            np.save(emb_cache, X_augmented)\n",
    "            np.save(lbl_cache, y_augmented)\n",
    "            np.save(paths_cache, paths_augmented)\n",
    "            print(\"\\n‚úÖ Cache files updated with incremental augmentations.\")\n",
    "\n",
    "    else:\n",
    "        # No cache exists: perform full augmentation up to TARGET_SAMPLES (same approach as incremental)\n",
    "        print(\"üîÑ No augmented cache found ‚Äî generating full augmented dataset up to TARGET_SAMPLES...\")\n",
    "        X_list = []\n",
    "        y_list = []\n",
    "        paths_list = []\n",
    "        augmentation_stats = []\n",
    "\n",
    "        for class_name in tqdm(unique_classes, desc='Processing classes'):\n",
    "            mask_orig = (y == class_name)\n",
    "            orig_embs = X[mask_orig]\n",
    "            orig_paths = np.array(paths)[mask_orig]\n",
    "            current_count = len(orig_embs)\n",
    "\n",
    "            # add originals\n",
    "            X_list.extend(orig_embs)\n",
    "            y_list.extend([class_name] * current_count)\n",
    "            paths_list.extend(orig_paths.tolist())\n",
    "\n",
    "            if current_count < TARGET_SAMPLES:\n",
    "                needed_total = TARGET_SAMPLES - current_count\n",
    "                synthetic_count = 0\n",
    "                emb_indices = np.arange(len(orig_embs))\n",
    "                syn_idx = 0\n",
    "\n",
    "                if len(emb_indices) == 0:\n",
    "                    print(f\"‚ö†Ô∏è Class '{class_name}' has 0 originals ‚Äî skipping.\")\n",
    "                    continue\n",
    "\n",
    "                while synthetic_count < needed_total:\n",
    "                    batch = int(min(AUG_BATCH, needed_total - synthetic_count))\n",
    "\n",
    "                    if len(emb_indices) == 1:\n",
    "                        idx1 = idx2 = np.zeros(batch, dtype=int)\n",
    "                    else:\n",
    "                        idx1 = np.random.choice(emb_indices, size=batch, replace=True)\n",
    "                        idx2 = np.random.choice(emb_indices, size=batch, replace=True)\n",
    "\n",
    "                    emb1 = orig_embs[idx1]\n",
    "                    emb2 = orig_embs[idx2]\n",
    "\n",
    "                    alphas = np.random.uniform(0.3, 0.7, size=(batch, 1))\n",
    "                    synthetics = alphas * emb1 + (1 - alphas) * emb2\n",
    "                    noise = np.random.normal(0.0, NOISE_STD, size=synthetics.shape)\n",
    "                    synthetics = synthetics + noise\n",
    "\n",
    "                    norms = np.linalg.norm(synthetics, axis=1, keepdims=True)\n",
    "                    norms[norms == 0] = 1.0\n",
    "                    synthetics = synthetics / norms\n",
    "\n",
    "                    for i in range(batch):\n",
    "                        X_list.append(synthetics[i].astype('float32'))\n",
    "                        y_list.append(class_name)\n",
    "                        paths_list.append(f\"synthetic_{class_name}_{syn_idx}\")\n",
    "                        syn_idx += 1\n",
    "\n",
    "                    synthetic_count += batch\n",
    "\n",
    "                augmentation_stats.append({\n",
    "                    'class': class_name,\n",
    "                    'original': current_count,\n",
    "                    'synthetic': synthetic_count,\n",
    "                    'total': current_count + synthetic_count\n",
    "                })\n",
    "\n",
    "        # convert and save\n",
    "        X_augmented = np.array(X_list, dtype='float32')\n",
    "        y_augmented = np.array(y_list, dtype=object)\n",
    "        paths_augmented = np.array(paths_list, dtype=object)\n",
    "\n",
    "        np.save(emb_cache, X_augmented)\n",
    "        np.save(lbl_cache, y_augmented)\n",
    "        np.save(paths_cache, paths_augmented)\n",
    "\n",
    "        if augmentation_stats:\n",
    "            aug_df = pd.DataFrame(augmentation_stats)\n",
    "            aug_df.to_csv(augmentation_report_csv, index=False)\n",
    "            print(\"\\n‚úÖ Augmentation complete and report saved.\")\n",
    "            print(aug_df.to_string(index=False))\n",
    "        else:\n",
    "            print(\"\\n‚ÑπÔ∏è No augmentation stats recorded (unexpected if dataset was imbalanced).\")\n",
    "\n",
    "    # ---- report final balance status ----\n",
    "    new_counts = Counter(np.load(lbl_cache, allow_pickle=True))\n",
    "    new_min = min(new_counts.values())\n",
    "    new_max = max(new_counts.values())\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä NEW BALANCE STATUS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Min samples per class: {new_min}\")\n",
    "    print(f\"Max samples per class: {new_max}\")\n",
    "    print(f\"New imbalance ratio: {new_max/new_min:.2f}x\")\n",
    "\n",
    "    if new_max / new_min <= 1.5:\n",
    "        print(\"‚úÖ Dataset is now BALANCED!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Still some imbalance (consider increasing TARGET_SAMPLES or re-running augmentation)\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìÅ AUGMENTED FILES SAVED:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"‚úÖ {emb_cache}\")\n",
    "    print(f\"‚úÖ {lbl_cache}\")\n",
    "    print(f\"‚úÖ {paths_cache}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚ö†Ô∏è IMPORTANT: Update Block 7 (training) to use augmented files:\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Change these lines in Block 7:\")\n",
    "    print(\"  FROM:\")\n",
    "    print(\"    EMB_SRC = EMB_FILE\")\n",
    "    print(\"    LBL_SRC = LBL_FILE\")\n",
    "    print(\"  TO:\")\n",
    "    print(\"    EMB_SRC = CACHE_DIR / 'X_emb_augmented.npy'\")\n",
    "    print(\"    LBL_SRC = CACHE_DIR / 'y_lbl_augmented.npy'\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚úÖ No augmentation performed. Dataset is already balanced!\")\n",
    "    print(\"   You can proceed directly to Block 7 (training).\")\n"
   ],
   "id": "21cfdfcaf1866b81"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59353f38bcf30c74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T17:44:09.670639Z",
     "start_time": "2025-11-13T17:42:58.564604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings: (19833, 512)\n",
      "Train/test: (16858, 512) (2975, 512)\n",
      "Trained SVM in 102.6s\n",
      "Saved classifier to embeddings_cache\\svc_model_retrained.pkl\n"
     ]
    }
   ],
   "source": [
    "# Block 7 ‚Äî train classifier (SVM)\n",
    "from sklearn.preprocessing import LabelEncoder, Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# choose embeddings source (deduped if you created them)\n",
    "EMB_SRC = CACHE_DIR / 'X_emb_augmented.npy'\n",
    "LBL_SRC = CACHE_DIR / 'y_lbl_augmented.npy'\n",
    "\n",
    "X = np.load(EMB_SRC)\n",
    "y = np.load(LBL_SRC, allow_pickle=True)\n",
    "print(\"Loaded embeddings:\", X.shape)\n",
    "\n",
    "le = LabelEncoder().fit(y); y_enc = le.transform(y)\n",
    "norm = Normalizer('l2'); Xn = norm.transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xn, y_enc, stratify=y_enc, test_size=0.15, random_state=42)\n",
    "print(\"Train/test:\", X_train.shape, X_test.shape)\n",
    "\n",
    "clf = SVC(kernel='linear', probability=True, class_weight='balanced')\n",
    "t0 = time.time(); clf.fit(X_train, y_train)\n",
    "print(\"Trained SVM in {:.1f}s\".format(time.time()-t0))\n",
    "\n",
    "# Save classifier with consistent keys\n",
    "with open(CLF_FILE, 'wb') as f:\n",
    "    pickle.dump({'clf': clf, 'le': le, 'norm': norm, 'X_test': X_test, 'y_test': y_test}, f)\n",
    "print(\"Saved classifier to\", CLF_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41df414e6464bafd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T17:52:17.112641Z",
     "start_time": "2025-11-13T17:45:24.192391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             precision    recall  f1-score   support\n",
      "\n",
      "          pins_Adriana Lima       1.00      1.00      1.00        32\n",
      "          pins_Alex Lawther       1.00      1.00      1.00        23\n",
      "    pins_Alexandra Daddario       0.97      1.00      0.99        34\n",
      "          pins_Alvaro Morte       1.00      1.00      1.00        30\n",
      "           pins_Amanda Crew       1.00      1.00      1.00        30\n",
      "          pins_Andy Samberg       1.00      1.00      1.00        29\n",
      "         pins_Anne Hathaway       0.97      1.00      0.98        30\n",
      "        pins_Anthony Mackie       1.00      1.00      1.00        30\n",
      "         pins_Avril Lavigne       1.00      0.96      0.98        24\n",
      "           pins_Ben Affleck       1.00      1.00      1.00        30\n",
      "            pins_Bill Gates       1.00      1.00      1.00        30\n",
      "          pins_Bobby Morley       0.97      1.00      0.98        30\n",
      "      pins_Brenton Thwaites       0.97      0.97      0.97        31\n",
      "        pins_Brian J. Smith       1.00      1.00      1.00        30\n",
      "           pins_Brie Larson       1.00      0.92      0.96        25\n",
      "           pins_Chris Evans       1.00      1.00      1.00        25\n",
      "       pins_Chris Hemsworth       1.00      1.00      1.00        24\n",
      "           pins_Chris Pratt       1.00      1.00      1.00        26\n",
      "        pins_Christian Bale       1.00      1.00      1.00        23\n",
      "     pins_Cristiano Ronaldo       1.00      1.00      1.00        30\n",
      "    pins_Danielle Panabaker       0.96      1.00      0.98        27\n",
      "       pins_Dominic Purcell       1.00      1.00      1.00        30\n",
      "        pins_Dwayne Johnson       1.00      1.00      1.00        30\n",
      "          pins_Eliza Taylor       1.00      1.00      1.00        24\n",
      "        pins_Elizabeth Lail       1.00      1.00      1.00        23\n",
      "         pins_Emilia Clarke       1.00      0.97      0.98        31\n",
      "            pins_Emma Stone       1.00      0.97      0.98        30\n",
      "           pins_Emma Watson       0.94      1.00      0.97        32\n",
      "       pins_Gwyneth Paltrow       1.00      1.00      1.00        28\n",
      "           pins_Henry Cavil       1.00      1.00      1.00        29\n",
      "          pins_Hugh Jackman       1.00      0.96      0.98        27\n",
      "            pins_Inbar Lavi       1.00      1.00      1.00        30\n",
      "           pins_Irina Shayk       0.96      1.00      0.98        23\n",
      "         pins_Jake Mcdorman       1.00      1.00      1.00        24\n",
      "           pins_Jason Momoa       1.00      1.00      1.00        28\n",
      "     pins_Jennifer Lawrence       0.96      1.00      0.98        27\n",
      "         pins_Jeremy Renner       1.00      1.00      1.00        25\n",
      "        pins_Jessica Barden       1.00      0.93      0.97        30\n",
      "          pins_Jimmy Fallon       1.00      1.00      1.00        30\n",
      "           pins_Johnny Depp       1.00      1.00      1.00        27\n",
      "           pins_Josh Radnor       1.00      1.00      1.00        30\n",
      "      pins_Katharine Mcphee       1.00      1.00      1.00        27\n",
      "    pins_Katherine Langford       1.00      1.00      1.00        34\n",
      "          pins_Keanu Reeves       1.00      1.00      1.00        24\n",
      "        pins_Krysten Ritter       1.00      1.00      1.00        26\n",
      "     pins_Leonardo DiCaprio       0.94      0.97      0.96        35\n",
      "         pins_Lili Reinhart       0.96      1.00      0.98        22\n",
      "        pins_Lindsey Morgan       0.93      1.00      0.96        25\n",
      "          pins_Lionel Messi       1.00      1.00      1.00        30\n",
      "          pins_Logan Lerman       1.00      0.97      0.98        32\n",
      "      pins_Madelaine Petsch       0.97      1.00      0.98        29\n",
      "       pins_Maisie Williams       1.00      1.00      1.00        29\n",
      "         pins_Maria Pedraza       1.00      1.00      1.00        30\n",
      "    pins_Marie Avgeropoulos       1.00      1.00      1.00        24\n",
      "          pins_Mark Ruffalo       1.00      1.00      1.00        27\n",
      "       pins_Mark Zuckerberg       1.00      1.00      1.00        30\n",
      "             pins_Megan Fox       1.00      1.00      1.00        31\n",
      "           pins_Miley Cyrus       1.00      1.00      1.00        27\n",
      "    pins_Millie Bobby Brown       0.97      1.00      0.98        29\n",
      "       pins_Morena Baccarin       1.00      1.00      1.00        26\n",
      "        pins_Morgan Freeman       1.00      1.00      1.00        30\n",
      "          pins_Nadia Hilker       1.00      1.00      1.00        30\n",
      "        pins_Natalie Dormer       1.00      1.00      1.00        29\n",
      "       pins_Natalie Portman       1.00      1.00      1.00        25\n",
      "   pins_Neil Patrick Harris       1.00      1.00      1.00        30\n",
      "          pins_Pedro Alonso       1.00      1.00      1.00        30\n",
      "          pins_Penn Badgley       1.00      1.00      1.00        26\n",
      "            pins_Rami Malek       1.00      1.00      1.00        24\n",
      "      pins_Rebecca Ferguson       1.00      1.00      1.00        27\n",
      "        pins_Richard Harmon       1.00      0.97      0.98        30\n",
      "               pins_Rihanna       1.00      1.00      1.00        30\n",
      "        pins_Robert De Niro       1.00      1.00      1.00        23\n",
      "      pins_Robert Downey Jr       1.00      1.00      1.00        35\n",
      "   pins_Sarah Wayne Callies       1.00      0.96      0.98        24\n",
      "          pins_Selena Gomez       1.00      0.96      0.98        28\n",
      "pins_Shakira Isabel Mebarak       1.00      1.00      1.00        23\n",
      "         pins_Sophie Turner       1.00      1.00      1.00        30\n",
      "         pins_Stephen Amell       1.00      1.00      1.00        24\n",
      "          pins_Taylor Swift       1.00      1.00      1.00        30\n",
      "            pins_Tom Cruise       1.00      1.00      1.00        29\n",
      "             pins_Tom Hardy       1.00      1.00      1.00        30\n",
      "        pins_Tom Hiddleston       1.00      1.00      1.00        27\n",
      "           pins_Tom Holland       1.00      0.96      0.98        28\n",
      "    pins_Tuppence Middleton       1.00      1.00      1.00        30\n",
      "        pins_Ursula Corbero       1.00      1.00      1.00        25\n",
      "      pins_Wentworth Miller       1.00      1.00      1.00        27\n",
      "             pins_Zac Efron       1.00      1.00      1.00        29\n",
      "               pins_Zendaya       1.00      0.97      0.98        30\n",
      "           pins_Zoe Saldana       1.00      0.96      0.98        28\n",
      "   pins_alycia dabnem carey       1.00      1.00      1.00        32\n",
      "           pins_amber heard       1.00      1.00      1.00        33\n",
      "          pins_barack obama       1.00      1.00      1.00        30\n",
      "        pins_barbara palvin       1.00      1.00      1.00        29\n",
      "         pins_camila mendes       1.00      1.00      1.00        24\n",
      "       pins_elizabeth olsen       1.00      0.94      0.97        33\n",
      "            pins_ellen page       0.93      1.00      0.97        28\n",
      "             pins_elon musk       1.00      1.00      1.00        30\n",
      "             pins_gal gadot       0.97      0.97      0.97        30\n",
      "          pins_grant gustin       1.00      1.00      1.00        27\n",
      "            pins_jeff bezos       1.00      1.00      1.00        30\n",
      "        pins_kiernen shipka       0.97      1.00      0.98        30\n",
      "         pins_margot robbie       0.97      0.97      0.97        33\n",
      "        pins_melissa fumero       1.00      1.00      1.00        23\n",
      "    pins_scarlett johansson       1.00      0.97      0.98        30\n",
      "             pins_tom ellis       0.97      1.00      0.99        34\n",
      "\n",
      "                   accuracy                           0.99      2975\n",
      "                  macro avg       0.99      0.99      0.99      2975\n",
      "               weighted avg       0.99      0.99      0.99      2975\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold CV: mean=0.9917 std=0.0013\n",
      "Saved centroids to embeddings_cache\\centroids.npy and classes to embeddings_cache\\classes.npy\n",
      "Centroid baseline accuracy: 0.9870771569745344\n",
      "Suggested cosine threshold for open-set (approx TPR=0.95): 0.4617\n"
     ]
    }
   ],
   "source": [
    "# Block 8 ‚Äî evaluation: classification report, confusion matrix, 5-fold CV, centroid baseline, threshold\n",
    "import pickle, numpy as np, matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# load saved classifier\n",
    "obj = pickle.load(open(CLF_FILE, 'rb'))\n",
    "clf = obj['clf']; le = obj['le']; norm = obj['norm']\n",
    "X_test = obj['X_test']; y_test = obj['y_test']\n",
    "\n",
    "# report\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred).astype(float)\n",
    "cmn = cm / cm.sum(axis=1)[:, None]\n",
    "plt.figure(figsize=(8,8)); plt.imshow(cmn); plt.title('Normalized confusion matrix'); plt.colorbar(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# 5-fold CV on full dataset\n",
    "X_all = np.load(EMB_FILE); y_all = np.load(LBL_FILE, allow_pickle=True)\n",
    "y_all_enc = le.transform(y_all); Xn_all = norm.transform(X_all)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accs = []\n",
    "for tr, te in skf.split(Xn_all, y_all_enc):\n",
    "    clf.fit(Xn_all[tr], y_all_enc[tr])\n",
    "    accs.append(clf.score(Xn_all[te], y_all_enc[te]))\n",
    "print(\"5-fold CV: mean={:.4f} std={:.4f}\".format(np.mean(accs), np.std(accs)))\n",
    "\n",
    "# centroid baseline\n",
    "Xn_norm = normalize(X_all, axis=1)\n",
    "centroids = {c: Xn_norm[y_all_enc==c].mean(axis=0) for c in np.unique(y_all_enc)}\n",
    "# save centroids and classes for lightweight inference\n",
    "centroid_matrix = np.vstack([centroids[c] for c in sorted(centroids.keys())])\n",
    "classes_order = np.array([le.classes_[c] for c in sorted(centroids.keys())])\n",
    "np.save(CENTROIDS_FILE, centroid_matrix)\n",
    "np.save(CLASSES_FILE, classes_order)\n",
    "print(\"Saved centroids to\", CENTROIDS_FILE, \"and classes to\", CLASSES_FILE)\n",
    "\n",
    "# centroid accuracy on same test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "_, X_test_c, _, y_test_c = train_test_split(Xn_norm, y_all_enc, stratify=y_all_enc, test_size=0.15, random_state=42)\n",
    "def predict_centroid(emb):\n",
    "    emb = emb / np.linalg.norm(emb)\n",
    "    sims = centroid_matrix.dot(emb)\n",
    "    return sims.argmax(), sims.max()\n",
    "y_cent = [predict_centroid(e)[0] for e in X_test_c]\n",
    "print(\"Centroid baseline accuracy:\", accuracy_score(y_test_c, y_cent))\n",
    "\n",
    "# Open-set threshold suggestion: compute genuine vs impostor cosine scores on validation set\n",
    "# Build per-class centroid from training portion and compute scores: simple approach using split\n",
    "tr, val = next(StratifiedKFold(n_splits=5, shuffle=True, random_state=42).split(Xn_all, y_all_enc))\n",
    "# compute centroids on tr\n",
    "centroids_tr = {c: Xn_all[tr][y_all_enc[tr]==c].mean(axis=0) for c in np.unique(y_all_enc)}\n",
    "# compute genuine scores and impostor scores on val set\n",
    "genuine_scores = []\n",
    "impostor_scores = []\n",
    "for i, emb in enumerate(Xn_all[val]):\n",
    "    label = y_all_enc[val][i]\n",
    "    emb_norm = emb / np.linalg.norm(emb)\n",
    "    sims = [emb_norm.dot(centroids_tr[c]) for c in centroids_tr]\n",
    "    # genuine score = sim with true class centroid\n",
    "    genuine_scores.append(emb_norm.dot(centroids_tr[label]))\n",
    "    # impostor score = max sim to any other class centroid\n",
    "    impostor_scores.append(max([s for c,s in zip(centroids_tr.keys(), sims) if c != label]))\n",
    "\n",
    "genuine_scores = np.array(genuine_scores); impostor_scores = np.array(impostor_scores)\n",
    "# compute ROC between genuine and impostor by labeling genuine=1, impostor=0 (stack arrays)\n",
    "labels_scores = np.concatenate([np.ones_like(genuine_scores), np.zeros_like(impostor_scores)])\n",
    "scores = np.concatenate([genuine_scores, impostor_scores])\n",
    "fpr, tpr, thresholds = roc_curve(labels_scores, scores)\n",
    "# pick threshold where tpr ~ 0.95 or where (tpr-fpr) is maximized\n",
    "idx = (np.abs(tpr - 0.95)).argmin()\n",
    "suggested_thresh = thresholds[idx]\n",
    "print(f\"Suggested cosine threshold for open-set (approx TPR=0.95): {suggested_thresh:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef136c659630fb96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T17:53:57.726706Z",
     "start_time": "2025-11-13T17:53:57.571345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference helpers ready. Example:\n"
     ]
    }
   ],
   "source": [
    "# Block 9.1 ‚Äî inference helpers (SVM and centroid)\n",
    "import numpy as np, pickle, cv2, torch\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# load artifacts\n",
    "obj = pickle.load(open(CLF_FILE, 'rb'))\n",
    "clf = obj['clf']; le = obj['le']; norm = obj['norm']\n",
    "centroid_matrix = np.load(CENTROIDS_FILE)\n",
    "classes_order = np.load(CLASSES_FILE)\n",
    "\n",
    "def predict_with_svm(image_path, top_k=3):\n",
    "    img = cv2.cvtColor(cv2.imread(str(image_path)), cv2.COLOR_BGR2RGB)\n",
    "    face = mtcnn(img)\n",
    "    if face is None:\n",
    "        return None\n",
    "    if face.dim()==3: face = face.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        emb = resnet(face).cpu().numpy().reshape(1,-1)\n",
    "    emb = norm.transform(emb)\n",
    "    probs = clf.predict_proba(emb)[0]\n",
    "    idx = probs.argsort()[::-1][:top_k]\n",
    "    return [(le.classes_[i], float(probs[i])) for i in idx]\n",
    "\n",
    "def predict_with_centroid(image_path, top_k=3):\n",
    "    img = cv2.cvtColor(cv2.imread(str(image_path)), cv2.COLOR_BGR2RGB)\n",
    "    face = mtcnn(img)\n",
    "    if face is None:\n",
    "        return None\n",
    "    if face.dim()==3: face = face.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        emb = resnet(face).cpu().numpy().reshape(-1)\n",
    "    emb = emb / np.linalg.norm(emb)\n",
    "    sims = centroid_matrix.dot(emb)\n",
    "    idx = sims.argsort()[::-1][:top_k]\n",
    "    return [(classes_order[i], float(sims[i])) for i in idx]\n",
    "\n",
    "print(\"Inference helpers ready. Example:\")\n",
    "# print(predict_with_centroid('path/to/example.jpg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48455e1a9b4a10b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T17:54:12.002355Z",
     "start_time": "2025-11-13T17:54:09.853371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroid accuracy on test: 0.985546218487395\n",
      "Centroid accuracy on test: 0.985546218487395\n"
     ]
    }
   ],
   "source": [
    "# Block 9.2\n",
    "\n",
    "# centroid baseline\n",
    "from sklearn.preprocessing import normalize\n",
    "X = np.load('./embeddings_cache/X_emb.npy'); y = np.load('./embeddings_cache/y_lbl.npy')\n",
    "le = LabelEncoder().fit(y); y_enc = le.transform(y)\n",
    "Xn = normalize(X, axis=1)\n",
    "centroids = {c: Xn[y_enc==c].mean(axis=0) for c in np.unique(y_enc)}\n",
    "\n",
    "# test set predictions (use previous X_test,y_test from split or re-split similarly)\n",
    "import numpy as np\n",
    "def predict_centroid(emb):\n",
    "    emb = emb/np.linalg.norm(emb)\n",
    "    sims = [(c, emb.dot(centroids[c])) for c in centroids] \n",
    "    return max(sims, key=lambda x: x[1])[0]\n",
    "\n",
    "y_cent = [predict_centroid(e) for e in X_test]  # X_test must be normalized rows\n",
    "# map centroids' indices -> label names (if needed)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Centroid accuracy on test:\", accuracy_score(y_test, y_cent))\n",
    "\n",
    "# centroid baseline\n",
    "from sklearn.preprocessing import normalize\n",
    "X = np.load('./embeddings_cache/X_emb.npy'); y = np.load('./embeddings_cache/y_lbl.npy')\n",
    "le = LabelEncoder().fit(y); y_enc = le.transform(y)\n",
    "Xn = normalize(X, axis=1)\n",
    "centroids = {c: Xn[y_enc==c].mean(axis=0) for c in np.unique(y_enc)}\n",
    "\n",
    "# test set predictions (use previous X_test,y_test from split or re-split similarly)\n",
    "import numpy as np\n",
    "def predict_centroid(emb):\n",
    "    emb = emb/np.linalg.norm(emb)\n",
    "    sims = [(c, emb.dot(centroids[c])) for c in centroids]\n",
    "    return max(sims, key=lambda x: x[1])[0]\n",
    "\n",
    "y_cent = [predict_centroid(e) for e in X_test]  # X_test must be normalized rows\n",
    "# map centroids' indices -> label names (if needed)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Centroid accuracy on test:\", accuracy_score(y_test, y_cent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "297a7c1f-a897-4d46-98ea-4fbcaff81f8b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-13T17:54:27.493176Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model...\n",
      "‚úÖ Model loaded. Can recognize 105 classes\n",
      "Found 17534 images in human_face_dataset/pins_face_recognition\n",
      "\n",
      "Processing images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17534/17534 [1:42:42<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä PREDICTION SUMMARY\n",
      "================================================================================\n",
      "‚úÖ Total images processed: 17486\n",
      "‚úÖ Correct predictions: 17442 (99.75%)\n",
      "‚ùå Wrong predictions: 44 (0.25%)\n",
      "üìä Overall Accuracy: 0.9975 (99.75%)\n",
      "üìä Average confidence: 0.8239\n",
      "‚ùå Failed to process: 48 images\n",
      "\n",
      "================================================================================\n",
      "‚ùå WRONG PREDICTIONS DETAILS (44 total)\n",
      "================================================================================\n",
      "\n",
      "Classes with wrong predictions:\n",
      "            actual_class  wrong_count                                                        predicted_as\n",
      "        pins_Brie Larson            4            pins_Emma Stone, pins_ellen page, pins_Jennifer Lawrence\n",
      "     pins_Jessica Barden            3     pins_Alex Lawther, pins_kiernen shipka, pins_Danielle Panabaker\n",
      "       pins_Logan Lerman            3 pins_Sarah Wayne Callies, pins_Leonardo DiCaprio, pins_Eliza Taylor\n",
      " pins_scarlett johansson            2                                   pins_gal gadot, pins_Taylor Swift\n",
      "      pins_Emilia Clarke            2                           pins_Marie Avgeropoulos, pins_Irina Shayk\n",
      "   pins_Brenton Thwaites            2                           pins_Leonardo DiCaprio, pins_Bobby Morley\n",
      "    pins_elizabeth olsen            2                            pins_ellen page, pins_Millie Bobby Brown\n",
      "            pins_Zendaya            2                               pins_Lili Reinhart, pins_Selena Gomez\n",
      "        pins_Tom Holland            2                           pins_Anne Hathaway, pins_Robert Downey Jr\n",
      "     pins_Tom Hiddleston            1                                                    pins_Chris Evans\n",
      "       pins_Selena Gomez            1                                                    pins_Emma Watson\n",
      "       pins_Taylor Swift            1                                                     pins_Emma Stone\n",
      "        pins_Zoe Saldana            1                                                 pins_Lindsey Morgan\n",
      "     pins_Richard Harmon            1                                              pins_Leonardo DiCaprio\n",
      "        pins_amber heard            1                                                    pins_Brie Larson\n",
      "          pins_gal gadot            1                                               pins_Madelaine Petsch\n",
      "      pins_margot robbie            1                                                    pins_Emma Watson\n",
      "pins_Sarah Wayne Callies            1                                                 pins_Lindsey Morgan\n",
      "      pins_Avril Lavigne            1                                             pins_Alexandra Daddario\n",
      "    pins_Natalie Portman            1                                             pins_Millie Bobby Brown\n",
      "       pins_Nadia Hilker            1                                             pins_Marie Avgeropoulos\n",
      " pins_Marie Avgeropoulos            1                                                    pins_Johnny Depp\n",
      "     pins_Lindsey Morgan            1                                                  pins_Anne Hathaway\n",
      "  pins_Leonardo DiCaprio            1                                               pins_Brenton Thwaites\n",
      " pins_Katherine Langford            1                                               pins_Madelaine Petsch\n",
      "        pins_Johnny Depp            1                                              pins_Leonardo DiCaprio\n",
      "        pins_Irina Shayk            1                                                  pins_Maria Pedraza\n",
      "       pins_Hugh Jackman            1                                                      pins_tom ellis\n",
      "         pins_Emma Stone            1                                                  pins_margot robbie\n",
      "    pins_Chris Hemsworth            1                                                    pins_Chris Evans\n",
      "    pins_Morena Baccarin            1                                                  pins_camila mendes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Individual wrong predictions (showing first 20):\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚Ä¢ amber heard214_323.jpg\n",
      "    Actual: pins_amber heard ‚Üí Predicted: pins_Brie Larson (confidence: 0.275)\n",
      "  ‚Ä¢ Avril Lavigne238_664.jpg\n",
      "    Actual: pins_Avril Lavigne ‚Üí Predicted: pins_Alexandra Daddario (confidence: 0.131)\n",
      "  ‚Ä¢ Brenton Thwaites46_885.jpg\n",
      "    Actual: pins_Brenton Thwaites ‚Üí Predicted: pins_Leonardo DiCaprio (confidence: 0.273)\n",
      "  ‚Ä¢ Brenton Thwaites99_936.jpg\n",
      "    Actual: pins_Brenton Thwaites ‚Üí Predicted: pins_Bobby Morley (confidence: 0.204)\n",
      "  ‚Ä¢ Brie Larson157_994.jpg\n",
      "    Actual: pins_Brie Larson ‚Üí Predicted: pins_Emma Stone (confidence: 0.136)\n",
      "  ‚Ä¢ Brie Larson172_1007.jpg\n",
      "    Actual: pins_Brie Larson ‚Üí Predicted: pins_ellen page (confidence: 0.253)\n",
      "  ‚Ä¢ Brie Larson187_1021.jpg\n",
      "    Actual: pins_Brie Larson ‚Üí Predicted: pins_Jennifer Lawrence (confidence: 0.127)\n",
      "  ‚Ä¢ Brie Larson77_1088.jpg\n",
      "    Actual: pins_Brie Larson ‚Üí Predicted: pins_margot robbie (confidence: 0.330)\n",
      "  ‚Ä¢ Chris Hemsworth1_384.jpg\n",
      "    Actual: pins_Chris Hemsworth ‚Üí Predicted: pins_Chris Evans (confidence: 0.095)\n",
      "  ‚Ä¢ elizabeth olsen164_1173.jpg\n",
      "    Actual: pins_elizabeth olsen ‚Üí Predicted: pins_ellen page (confidence: 0.355)\n",
      "  ‚Ä¢ elizabeth olsen170_1179.jpg\n",
      "    Actual: pins_elizabeth olsen ‚Üí Predicted: pins_Millie Bobby Brown (confidence: 0.272)\n",
      "  ‚Ä¢ Emilia Clarke194_952.jpg\n",
      "    Actual: pins_Emilia Clarke ‚Üí Predicted: pins_Marie Avgeropoulos (confidence: 0.150)\n",
      "  ‚Ä¢ Emilia Clarke48_1021.jpg\n",
      "    Actual: pins_Emilia Clarke ‚Üí Predicted: pins_Irina Shayk (confidence: 0.069)\n",
      "  ‚Ä¢ Emma Stone36_1779.jpg\n",
      "    Actual: pins_Emma Stone ‚Üí Predicted: pins_margot robbie (confidence: 0.282)\n",
      "  ‚Ä¢ gal gadot134_1690.jpg\n",
      "    Actual: pins_gal gadot ‚Üí Predicted: pins_Madelaine Petsch (confidence: 0.234)\n",
      "  ‚Ä¢ Hugh Jackman118_1288.jpg\n",
      "    Actual: pins_Hugh Jackman ‚Üí Predicted: pins_tom ellis (confidence: 0.128)\n",
      "  ‚Ä¢ Irina Shayk236_2335.jpg\n",
      "    Actual: pins_Irina Shayk ‚Üí Predicted: pins_Maria Pedraza (confidence: 0.082)\n",
      "  ‚Ä¢ Jessica Barden211_1449.jpg\n",
      "    Actual: pins_Jessica Barden ‚Üí Predicted: pins_Alex Lawther (confidence: 0.779)\n",
      "  ‚Ä¢ Jessica Barden31_1475.jpg\n",
      "    Actual: pins_Jessica Barden ‚Üí Predicted: pins_kiernen shipka (confidence: 0.098)\n",
      "  ‚Ä¢ Jessica Barden34_1478.jpg\n",
      "    Actual: pins_Jessica Barden ‚Üí Predicted: pins_Danielle Panabaker (confidence: 0.048)\n",
      "\n",
      "  ... and 24 more wrong predictions (see CSV for details)\n",
      "\n",
      "================================================================================\n",
      "üéØ CLASSES WITH 100% ACCURACY (74 classes)\n",
      "================================================================================\n",
      "              class_name  total_count\n",
      "       pins_Adriana Lima          213\n",
      " pins_Millie Bobby Brown          191\n",
      "            pins_Rihanna          132\n",
      "   pins_Rebecca Ferguson          178\n",
      "         pins_Rami Malek          160\n",
      "       pins_Penn Badgley          171\n",
      "       pins_Pedro Alonso          125\n",
      "pins_Neil Patrick Harris          116\n",
      "     pins_Natalie Dormer          196\n",
      "     pins_Morgan Freeman          102\n",
      "        pins_Miley Cyrus          178\n",
      "       pins_Keanu Reeves          158\n",
      "          pins_Megan Fox          208\n",
      "    pins_Mark Zuckerberg           95\n",
      "       pins_Mark Ruffalo          177\n",
      "       pins_Alex Lawther          152\n",
      "    pins_Maisie Williams          193\n",
      "   pins_Madelaine Petsch          192\n",
      "       pins_Lionel Messi           86\n",
      "      pins_Lili Reinhart          150\n",
      "... and 54 more classes\n",
      "\n",
      "================================================================================\n",
      "‚ö†Ô∏è CLASSES WITH LOWEST ACCURACY (Bottom 10)\n",
      "================================================================================\n",
      "             class_name  correct_count  wrong_count  total_count  accuracy\n",
      "      pins_Taylor Swift            129            1          130  0.992308\n",
      "   pins_elizabeth olsen            219            2          221  0.990950\n",
      "  pins_Brenton Thwaites            207            2          209  0.990431\n",
      "     pins_Emilia Clarke            207            2          209  0.990431\n",
      "pins_scarlett johansson            199            2          201  0.990050\n",
      "       pins_Tom Holland            187            2          189  0.989418\n",
      "      pins_Logan Lerman            209            3          212  0.985849\n",
      "           pins_Zendaya            135            2          137  0.985401\n",
      "    pins_Jessica Barden            138            3          141  0.978723\n",
      "       pins_Brie Larson            165            4          169  0.976331\n",
      "\n",
      "================================================================================\n",
      "‚úÖ CORRECT PREDICTIONS SAMPLE (showing 10 of 17442)\n",
      "================================================================================\n",
      "  ‚úì Adriana Lima0_0.jpg: pins_Adriana Lima (confidence: 0.787)\n",
      "  ‚úì Adriana Lima101_3.jpg: pins_Adriana Lima (confidence: 0.946)\n",
      "  ‚úì Adriana Lima102_4.jpg: pins_Adriana Lima (confidence: 0.907)\n",
      "  ‚úì Adriana Lima103_5.jpg: pins_Adriana Lima (confidence: 0.752)\n",
      "  ‚úì Adriana Lima104_6.jpg: pins_Adriana Lima (confidence: 0.886)\n",
      "  ‚úì Adriana Lima105_7.jpg: pins_Adriana Lima (confidence: 0.779)\n",
      "  ‚úì Adriana Lima106_8.jpg: pins_Adriana Lima (confidence: 0.794)\n",
      "  ‚úì Adriana Lima107_9.jpg: pins_Adriana Lima (confidence: 0.930)\n",
      "  ‚úì Adriana Lima108_10.jpg: pins_Adriana Lima (confidence: 0.902)\n",
      "  ‚úì Adriana Lima109_11.jpg: pins_Adriana Lima (confidence: 0.375)\n",
      "\n",
      "================================================================================\n",
      "üìÅ OUTPUT FILES SAVED:\n",
      "================================================================================\n",
      "‚úÖ predictions_results.csv\n",
      "   ‚Üí All predictions sorted (correct first, then wrong)\n",
      "   ‚Üí Columns: filename, actual, predicted, confidence, status, top3\n",
      "\n",
      "‚úÖ predictions_summary.csv\n",
      "   ‚Üí Per-class accuracy summary\n",
      "   ‚Üí Columns: class_name, correct_count, wrong_count, total_count, accuracy\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "‚ùå FAILED TO PROCESS (48 images)\n",
      "================================================================================\n",
      "  ‚Ä¢ Anne Hathaway203_391.jpg: No face detected\n",
      "  ‚Ä¢ Avril Lavigne11_572.jpg: No face detected\n",
      "  ‚Ä¢ Avril Lavigne174_619.jpg: No face detected\n",
      "  ‚Ä¢ Avril Lavigne41_685.jpg: No face detected\n",
      "  ‚Ä¢ barbara palvin158_800.jpg: No face detected\n",
      "  ‚Ä¢ Cristiano Ronaldo209_1326.jpg: No face detected\n",
      "  ‚Ä¢ Cristiano Ronaldo226_1333.jpg: No face detected\n",
      "  ‚Ä¢ Eliza Taylor202_775.jpg: No face detected\n",
      "  ‚Ä¢ Elizabeth Lail102_1055.jpg: No face detected\n",
      "  ‚Ä¢ Elizabeth Lail102_1056.jpg: No face detected\n",
      "  ‚Ä¢ Elizabeth Lail194_1117.jpg: No face detected\n",
      "  ‚Ä¢ Emilia Clarke78_1050.jpg: No face detected\n",
      "  ‚Ä¢ Emma Stone73_1817.jpg: No face detected\n",
      "  ‚Ä¢ Hugh Jackman119_1289.jpg: No face detected\n",
      "  ‚Ä¢ jeff bezos112_2049.jpg: No face detected\n",
      "  ‚Ä¢ jeff bezos12_2052.jpg: No face detected\n",
      "  ‚Ä¢ jeff bezos160_2068.jpg: No face detected\n",
      "  ‚Ä¢ jeff bezos178_2073.jpg: No face detected\n",
      "  ‚Ä¢ Jeremy Renner175_2634.jpg: No face detected\n",
      "  ‚Ä¢ Johnny Depp23_1863.jpg: No face detected\n",
      "  ... and 28 more\n",
      "\n",
      "‚úÖ Failed images list saved to: failed_predictions.csv\n",
      "\n",
      "================================================================================\n",
      "‚úÖ PROCESSING COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Block 10 - Predict on images and generate detailed CSV report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# ---------- EDIT THIS: Path to folder containing images to predict ----------\n",
    "PREDICT_DIR = \"human_face_dataset/pins_face_recognition\"  # << CHANGE THIS to your test images folder\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Output CSV paths\n",
    "OUTPUT_CSV = \"predictions_results.csv\"\n",
    "OUTPUT_SUMMARY_CSV = \"predictions_summary.csv\"\n",
    "\n",
    "# Load trained model and artifacts\n",
    "print(\"Loading trained model...\")\n",
    "obj = pickle.load(open(CLF_FILE, 'rb'))\n",
    "clf = obj['clf']\n",
    "le = obj['le']\n",
    "norm = obj['norm']\n",
    "print(f\"‚úÖ Model loaded. Can recognize {len(le.classes_)} classes\")\n",
    "\n",
    "# Get all image files\n",
    "PREDICT_PATH = Path(PREDICT_DIR)\n",
    "image_paths = [str(p) for p in sorted(PREDICT_PATH.rglob('*')) if p.suffix.lower() in EXTS]\n",
    "print(f\"Found {len(image_paths)} images in {PREDICT_DIR}\")\n",
    "\n",
    "if len(image_paths) == 0:\n",
    "    print(\"‚ö†Ô∏è No images found! Check PREDICT_DIR path.\")\n",
    "else:\n",
    "    # Prepare results storage\n",
    "    results = []\n",
    "    failed_images = []\n",
    "\n",
    "    print(\"\\nProcessing images...\")\n",
    "    for img_path in tqdm(image_paths, desc='Predicting'):\n",
    "        try:\n",
    "            # Load and process image\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "            # Resize if too large (speed optimization)\n",
    "            w, h = img.size\n",
    "            max_side = 640\n",
    "            if max(w, h) > max_side:\n",
    "                scale = max_side / max(w, h)\n",
    "                img = img.resize((int(w*scale), int(h*scale)), Image.BILINEAR)\n",
    "\n",
    "            # Detect face\n",
    "            face = mtcnn(img)\n",
    "\n",
    "            if face is None:\n",
    "                failed_images.append((img_path, \"No face detected\"))\n",
    "                continue\n",
    "\n",
    "            # Ensure correct dimensions\n",
    "            if face.dim() == 3:\n",
    "                face = face.unsqueeze(0)\n",
    "\n",
    "            # Get embedding\n",
    "            with torch.no_grad():\n",
    "                emb = resnet(face).cpu().numpy().reshape(1, -1)\n",
    "\n",
    "            # Normalize\n",
    "            emb_norm = norm.transform(emb)\n",
    "\n",
    "            # Predict\n",
    "            pred_class = clf.predict(emb_norm)[0]\n",
    "            pred_label = le.inverse_transform([pred_class])[0]\n",
    "\n",
    "            # Get confidence (probability)\n",
    "            proba = clf.predict_proba(emb_norm)[0]\n",
    "            confidence = float(proba[pred_class])\n",
    "\n",
    "            # Get top 3 predictions\n",
    "            top3_idx = proba.argsort()[::-1][:3]\n",
    "            top3_labels = le.inverse_transform(top3_idx)\n",
    "            top3_probs = proba[top3_idx]\n",
    "\n",
    "            # Get actual label (from parent folder name)\n",
    "            actual_label = Path(img_path).parent.name\n",
    "\n",
    "            # Check if correct\n",
    "            is_correct = (actual_label == pred_label)\n",
    "\n",
    "            # Store result\n",
    "            results.append({\n",
    "                'image_path': img_path,\n",
    "                'filename': Path(img_path).name,\n",
    "                'actual': actual_label,\n",
    "                'predicted': pred_label,\n",
    "                'confidence': confidence,\n",
    "                'correct': is_correct,\n",
    "                'status': 'CORRECT ‚úì' if is_correct else 'WRONG ‚úó',\n",
    "                'top1': top3_labels[0],\n",
    "                'top1_conf': float(top3_probs[0]),\n",
    "                'top2': top3_labels[1] if len(top3_labels) > 1 else '',\n",
    "                'top2_conf': float(top3_probs[1]) if len(top3_probs) > 1 else 0.0,\n",
    "                'top3': top3_labels[2] if len(top3_labels) > 2 else '',\n",
    "                'top3_conf': float(top3_probs[2]) if len(top3_probs) > 2 else 0.0\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            failed_images.append((img_path, str(e)))\n",
    "\n",
    "    # Create DataFrame\n",
    "    if results:\n",
    "        results_df = pd.DataFrame(results)\n",
    "\n",
    "        # Sort: Correct predictions first (sorted by confidence desc), then wrong predictions\n",
    "        results_df_sorted = pd.concat([\n",
    "            results_df[results_df['correct']].sort_values('confidence', ascending=False),\n",
    "            results_df[~results_df['correct']].sort_values('confidence', ascending=False)\n",
    "        ])\n",
    "\n",
    "        # Save complete results to CSV\n",
    "        results_df_sorted.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "        # Calculate statistics\n",
    "        total_images = len(results)\n",
    "        correct_count = results_df['correct'].sum()\n",
    "        wrong_count = total_images - correct_count\n",
    "        accuracy = correct_count / total_images if total_images > 0 else 0\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìä PREDICTION SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"‚úÖ Total images processed: {total_images}\")\n",
    "        print(f\"‚úÖ Correct predictions: {correct_count} ({correct_count/total_images*100:.2f}%)\")\n",
    "        print(f\"‚ùå Wrong predictions: {wrong_count} ({wrong_count/total_images*100:.2f}%)\")\n",
    "        print(f\"üìä Overall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "        print(f\"üìä Average confidence: {results_df['confidence'].mean():.4f}\")\n",
    "        print(f\"‚ùå Failed to process: {len(failed_images)} images\")\n",
    "\n",
    "        # Per-class accuracy summary\n",
    "        class_summary = results_df.groupby('actual').agg({\n",
    "            'correct': ['sum', 'count', 'mean']\n",
    "        }).reset_index()\n",
    "        class_summary.columns = ['class_name', 'correct_count', 'total_count', 'accuracy']\n",
    "        class_summary['wrong_count'] = class_summary['total_count'] - class_summary['correct_count']\n",
    "        class_summary = class_summary.sort_values('accuracy', ascending=False)\n",
    "\n",
    "        # Save class summary\n",
    "        class_summary.to_csv(OUTPUT_SUMMARY_CSV, index=False)\n",
    "\n",
    "        # Show wrong predictions details\n",
    "        wrong_predictions = results_df[~results_df['correct']].copy()\n",
    "\n",
    "        if len(wrong_predictions) > 0:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(f\"‚ùå WRONG PREDICTIONS DETAILS ({len(wrong_predictions)} total)\")\n",
    "            print(\"=\"*80)\n",
    "\n",
    "            # Group wrong predictions by actual class\n",
    "            wrong_by_class = wrong_predictions.groupby('actual').agg({\n",
    "                'filename': 'count',\n",
    "                'predicted': lambda x: ', '.join(x.value_counts().head(3).index.tolist())\n",
    "            }).reset_index()\n",
    "            wrong_by_class.columns = ['actual_class', 'wrong_count', 'predicted_as']\n",
    "            wrong_by_class = wrong_by_class.sort_values('wrong_count', ascending=False)\n",
    "\n",
    "            print(\"\\nClasses with wrong predictions:\")\n",
    "            print(wrong_by_class.to_string(index=False))\n",
    "\n",
    "            print(\"\\n\" + \"-\"*80)\n",
    "            print(\"Individual wrong predictions (showing first 20):\")\n",
    "            print(\"-\"*80)\n",
    "            wrong_display = wrong_predictions[['filename', 'actual', 'predicted', 'confidence']].head(20)\n",
    "            for idx, row in wrong_display.iterrows():\n",
    "                print(f\"  ‚Ä¢ {row['filename']}\")\n",
    "                print(f\"    Actual: {row['actual']} ‚Üí Predicted: {row['predicted']} (confidence: {row['confidence']:.3f})\")\n",
    "\n",
    "            if len(wrong_predictions) > 20:\n",
    "                print(f\"\\n  ... and {len(wrong_predictions)-20} more wrong predictions (see CSV for details)\")\n",
    "        else:\n",
    "            print(\"\\nüéâ NO WRONG PREDICTIONS! Perfect accuracy!\")\n",
    "\n",
    "        # Show classes with perfect accuracy\n",
    "        perfect_classes = class_summary[class_summary['accuracy'] == 1.0]\n",
    "        if len(perfect_classes) > 0:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(f\"üéØ CLASSES WITH 100% ACCURACY ({len(perfect_classes)} classes)\")\n",
    "            print(\"=\"*80)\n",
    "            print(perfect_classes[['class_name', 'total_count']].head(20).to_string(index=False))\n",
    "            if len(perfect_classes) > 20:\n",
    "                print(f\"... and {len(perfect_classes)-20} more classes\")\n",
    "\n",
    "        # Show classes with lowest accuracy\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚ö†Ô∏è CLASSES WITH LOWEST ACCURACY (Bottom 10)\")\n",
    "        print(\"=\"*80)\n",
    "        print(class_summary[['class_name', 'correct_count', 'wrong_count', 'total_count', 'accuracy']].tail(10).to_string(index=False))\n",
    "\n",
    "        # Show correct predictions sample\n",
    "        correct_predictions = results_df[results_df['correct']]\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"‚úÖ CORRECT PREDICTIONS SAMPLE (showing 10 of {len(correct_predictions)})\")\n",
    "        print(\"=\"*80)\n",
    "        correct_display = correct_predictions[['filename', 'actual', 'confidence']].head(10)\n",
    "        for idx, row in correct_display.iterrows():\n",
    "            print(f\"  ‚úì {row['filename']}: {row['actual']} (confidence: {row['confidence']:.3f})\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìÅ OUTPUT FILES SAVED:\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"‚úÖ {OUTPUT_CSV}\")\n",
    "        print(f\"   ‚Üí All predictions sorted (correct first, then wrong)\")\n",
    "        print(f\"   ‚Üí Columns: filename, actual, predicted, confidence, status, top3\")\n",
    "        print(f\"\\n‚úÖ {OUTPUT_SUMMARY_CSV}\")\n",
    "        print(f\"   ‚Üí Per-class accuracy summary\")\n",
    "        print(f\"   ‚Üí Columns: class_name, correct_count, wrong_count, total_count, accuracy\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "    # Show failed images\n",
    "    if failed_images:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"‚ùå FAILED TO PROCESS ({len(failed_images)} images)\")\n",
    "        print(\"=\"*80)\n",
    "        for path, reason in failed_images[:20]:\n",
    "            print(f\"  ‚Ä¢ {Path(path).name}: {reason}\")\n",
    "        if len(failed_images) > 20:\n",
    "            print(f\"  ... and {len(failed_images)-20} more\")\n",
    "\n",
    "        # Save failed images list\n",
    "        failed_df = pd.DataFrame(failed_images, columns=['image_path', 'error'])\n",
    "        failed_df.to_csv('failed_predictions.csv', index=False)\n",
    "        print(f\"\\n‚úÖ Failed images list saved to: failed_predictions.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ PROCESSING COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff8b5ec3bf18af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34fec9d3eb4e0ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_face_identify)",
   "language": "python",
   "name": "venv_face_identify"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
